{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:00:01.617691Z",
     "start_time": "2025-04-29T00:59:59.903795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ----------------------------------\n",
    "# 🔧 텍스트 전처리 함수\n",
    "# ----------------------------------\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # HTML/XML 제거\n",
    "    if isinstance(text, str) and ('<' in text and '>' in text):\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\\(.*?\\\\\\)\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ----------------------------------\n",
    "# 📦 전처리 및 벡터화\n",
    "# ----------------------------------\n",
    "def preprocess_and_vectorize(df, method='count', max_features=5000, data_type='journal'):\n",
    "    if data_type == 'journal':\n",
    "        text_columns = ['title', 'abstract', 'keywords']\n",
    "    elif data_type == 'article':\n",
    "        text_columns = ['title', 'content', 'keywords']\n",
    "    else:\n",
    "        raise ValueError(\"data_type은 'journal' 또는 'article' 중 하나여야 합니다.\")\n",
    "\n",
    "    for col in text_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "        df[f'{col}_clean'] = df[col].apply(preprocess_text)\n",
    "\n",
    "    df['combined_text'] = df[[f'{col}_clean' for col in text_columns]].agg(' '.join, axis=1)\n",
    "\n",
    "    if method == 'count':\n",
    "        vectorizer = CountVectorizer(max_features=max_features, stop_words='english', max_df=0.90)\n",
    "    elif method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english', max_df=0.90)\n",
    "    else:\n",
    "        raise ValueError(\"method는 'count' 또는 'tfidf' 중 하나여야 합니다.\")\n",
    "\n",
    "    vectorized_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "    return vectorizer, vectorized_matrix\n",
    "\n",
    "# ----------------------------------\n",
    "# 🔍 문서 준비 및 토큰화\n",
    "# ----------------------------------\n",
    "def prepare_documents(df, data_type='journal'):\n",
    "    df = df.fillna('')\n",
    "\n",
    "    if 'combined_text' not in df.columns:\n",
    "        if data_type == 'journal':\n",
    "            text_columns = ['title', 'abstract', 'keywords']\n",
    "        elif data_type == 'article':\n",
    "            text_columns = ['title', 'content', 'keywords']\n",
    "        else:\n",
    "            raise ValueError(\"data_type은 'journal' 또는 'article' 중 하나여야 합니다.\")\n",
    "\n",
    "        for col in text_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = ''\n",
    "        df['combined_text'] = df[text_columns].agg(' '.join, axis=1)\n",
    "        df['combined_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "    return [doc.split() for doc in df['combined_text']]\n",
    "\n",
    "# ----------------------------------\n",
    "# 📈 Coherence 점수 계산\n",
    "# (여기서는 안쓰지만 남겨둠)\n",
    "# ----------------------------------\n",
    "def compute_coherence_scores(dictionary, corpus, texts, start, limit, step):\n",
    "    scores = []\n",
    "    for k in range(start, limit, step):\n",
    "        lda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42, passes=10)\n",
    "        cm = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        scores.append((k, cm.get_coherence()))\n",
    "    return scores\n",
    "\n",
    "# ----------------------------------\n",
    "# 🧩 토픽-문서 행렬 생성\n",
    "# ----------------------------------\n",
    "def extract_topic_matrix(lda_model, corpus, num_topics):\n",
    "    topic_matrix = []\n",
    "    for doc in corpus:\n",
    "        dist = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "        topic_matrix.append([prob for _, prob in sorted(dist)])\n",
    "    return pd.DataFrame(topic_matrix, columns=[f\"Topic_{i}\" for i in range(num_topics)])\n",
    "\n",
    "# ----------------------------------\n",
    "# 📊 요인 분석\n",
    "# ----------------------------------\n",
    "def run_factor_analysis(topic_df, n_factors=5, max_iter=500):\n",
    "    fa = FactorAnalysis(n_components=n_factors, random_state=42, max_iter=max_iter)\n",
    "    factors = fa.fit_transform(topic_df)\n",
    "    loadings = pd.DataFrame(fa.components_.T, index=topic_df.columns, columns=[f\"Factor_{i+1}\" for i in range(n_factors)])\n",
    "    return pd.DataFrame(factors, columns=loadings.columns), loadings\n",
    "\n",
    "# ----------------------------------\n",
    "# 📊 요인 분석 상위 문서 5개 저장\n",
    "# ----------------------------------\n",
    "# ----------------------------------\n",
    "# 📊 요인 분석 상위 문서 5개 저장 (500글자 제한 추가)\n",
    "# ----------------------------------\n",
    "def top_docs_by_factor(factor_df, docs_df, top_n=5, output_path='top_documents_by_factor.txt', data_type='journal'):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for factor in factor_df.columns:\n",
    "            f.write(f\"\\n📌 상위 문서 - {factor}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            top_indices = factor_df[factor].nlargest(top_n).index\n",
    "\n",
    "            for i in top_indices:\n",
    "                row = docs_df.loc[i]\n",
    "\n",
    "                f.write(f\"📆 Date: {row.get('date', '')}\\n\")\n",
    "                f.write(f\"📄 Title: {row.get('title', '')}\\n\")\n",
    "\n",
    "                if data_type == 'journal':\n",
    "                    content = row.get('abstract', '')\n",
    "                elif data_type == 'article':\n",
    "                    content = row.get('content', '')\n",
    "                else:\n",
    "                    raise ValueError(\"data_type은 'journal' 또는 'article' 중 하나여야 합니다.\")\n",
    "\n",
    "                # 🔵 본문 500글자까지만 저장\n",
    "                if len(content) > 500:\n",
    "                    content = content[:500].rstrip() + \"...\"\n",
    "\n",
    "                f.write(f\"🔍 Content (500자 이내): {content}\\n\")\n",
    "                f.write(f\"🏷️ Keywords: {row.get('keywords', '')}\\n\")\n",
    "                f.write(\"-\"*60 + \"\\n\")\n",
    "\n",
    "    print(f\"✅ 저장 완료 (본문 500자 제한 적용): {output_path}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# 📝 LDA 모델로부터 토픽별 키워드 저장\n",
    "# ----------------------------------\n",
    "def save_lda_topics(lda_model, num_words, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for idx, topic in lda_model.show_topics(num_topics=-1, num_words=num_words, formatted=False):\n",
    "            keywords = \", \".join([word for word, _ in topic])\n",
    "            f.write(f\"Topic {idx}: {keywords}\\n\")\n",
    "    print(f\"✅ LDA 토픽 저장 완료: {output_path}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# 🎯 년도별로 LDA + Factor 분석\n",
    "# ----------------------------------\n",
    "def run_yearly_lda_factor_analysis(df, data_type='journal', n_topics=10, n_factors=5, vectorizer_method='tfidf', max_features=5000, output_dir='results'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    years = sorted(df['date'].unique())\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"🔵 Processing year: {year}\")\n",
    "\n",
    "        # 해당 연도 데이터 추출\n",
    "        year_df = df[df['date'] == year].reset_index(drop=True)\n",
    "\n",
    "        # 벡터화\n",
    "        vectorizer, vectorized_matrix = preprocess_and_vectorize(year_df, method=vectorizer_method, max_features=max_features, data_type=data_type)\n",
    "\n",
    "        # Gensim LDA용 corpus 준비\n",
    "        processed_docs = prepare_documents(year_df, data_type=data_type)\n",
    "        dictionary = corpora.Dictionary(processed_docs)\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "        # LDA 모델 학습\n",
    "        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics, random_state=42, passes=10)\n",
    "\n",
    "        # 🎯 LDA 토픽별 키워드 저장 추가\n",
    "        save_lda_topics(\n",
    "            lda_model=lda_model,\n",
    "            num_words=10,  # 토픽당 상위 10개 단어\n",
    "            output_path=f\"{output_dir}/02_{data_type}_{year}_lda_topics.txt\"\n",
    "        )\n",
    "\n",
    "        # 토픽-문서 행렬 생성\n",
    "        topic_df = extract_topic_matrix(lda_model, corpus, n_topics)\n",
    "\n",
    "        # Factor Analysis\n",
    "        factor_df, loadings = run_factor_analysis(topic_df, n_factors=n_factors)\n",
    "\n",
    "        # 결과 저장\n",
    "        topic_df.to_csv(f\"{output_dir}/02_{data_type}_{year}_topic_matrix.csv\", index=False)\n",
    "        factor_df.to_csv(f\"{output_dir}/02_{data_type}{year}_factor_scores.csv\", index=False)\n",
    "        loadings.to_csv(f\"{output_dir}/02_{data_type}{year}_factor_loadings.csv\", index=True)\n",
    "\n",
    "        top_docs_by_factor(\n",
    "            factor_df=factor_df,\n",
    "            docs_df=year_df,\n",
    "            top_n=5,\n",
    "            output_path=f\"{output_dir}/02_{data_type}_{year}_top_docs_by_factor.txt\",  # 여기 수정\n",
    "            data_type=data_type\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Year {year} 완료! (Topic Matrix, Factor Scores, Loadings, Top Docs 저장)\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:00:01.830824Z",
     "start_time": "2025-04-29T01:00:01.620503Z"
    }
   },
   "cell_type": "code",
   "source": "df=pd.read_csv('data/01_combined_article.csv')",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:07:35.583031Z",
     "start_time": "2025-04-29T01:00:01.893415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_yearly_lda_factor_analysis(\n",
    "    df,  # 👉 당신이 만든 데이터프레임\n",
    "    data_type='article',  # 👉 논문이면 'journal', 뉴스 기사면 'article'\n",
    "    n_topics=10,          # 👉 고정: LDA 토픽 개수\n",
    "    n_factors=5,          # 👉 고정: Factor Analysis 요인 수\n",
    "    vectorizer_method='tfidf',  # 👉 'count'나 'tfidf' 중 선택 ('tfidf' 추천)\n",
    "    max_features=5000,     # 👉 벡터라이저 최대 단어 수 (선택사항)\n",
    "    output_dir='data/result/02'   # 👉 결과 저장 폴더명\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Processing year: 2021\n",
      "✅ LDA 토픽 저장 완료: data/result/02/02_article_2021_lda_topics.txt\n",
      "✅ 저장 완료 (본문 500자 제한 적용): data/result/02/02_article_2021_top_docs_by_factor.txt\n",
      "✅ Year 2021 완료! (Topic Matrix, Factor Scores, Loadings, Top Docs 저장)\n",
      "🔵 Processing year: 2022\n",
      "✅ LDA 토픽 저장 완료: data/result/02/02_article_2022_lda_topics.txt\n",
      "✅ 저장 완료 (본문 500자 제한 적용): data/result/02/02_article_2022_top_docs_by_factor.txt\n",
      "✅ Year 2022 완료! (Topic Matrix, Factor Scores, Loadings, Top Docs 저장)\n",
      "🔵 Processing year: 2023\n",
      "✅ LDA 토픽 저장 완료: data/result/02/02_article_2023_lda_topics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_news/lib/python3.11/site-packages/sklearn/decomposition/_factor_analysis.py:296: ConvergenceWarning: FactorAnalysis did not converge. You might want to increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료 (본문 500자 제한 적용): data/result/02/02_article_2023_top_docs_by_factor.txt\n",
      "✅ Year 2023 완료! (Topic Matrix, Factor Scores, Loadings, Top Docs 저장)\n",
      "🔵 Processing year: 2024\n",
      "✅ LDA 토픽 저장 완료: data/result/02/02_article_2024_lda_topics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_news/lib/python3.11/site-packages/sklearn/decomposition/_factor_analysis.py:296: ConvergenceWarning: FactorAnalysis did not converge. You might want to increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료 (본문 500자 제한 적용): data/result/02/02_article_2024_top_docs_by_factor.txt\n",
      "✅ Year 2024 완료! (Topic Matrix, Factor Scores, Loadings, Top Docs 저장)\n",
      "🔵 Processing year: 2025\n",
      "✅ LDA 토픽 저장 완료: data/result/02/02_article_2025_lda_topics.txt\n",
      "✅ 저장 완료 (본문 500자 제한 적용): data/result/02/02_article_2025_top_docs_by_factor.txt\n",
      "✅ Year 2025 완료! (Topic Matrix, Factor Scores, Loadings, Top Docs 저장)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 데이터 라벨링 작업 - 토픽모델링"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2021년\n",
    "- Google Search: Topic 0: google, company, new, also, data, year, like, search, said, user\n",
    "- Facial Recognition: Topic 1: recognition, facial, system, uber, company, like, technology, driver, model, would\n",
    "- Customer Features: Topic 2: also, company, new, like, user, customer, feature, say, time, million\n",
    "- Startup: Topic 3: company, people, like, startup, tech, think, year, one, work, automation\n",
    "- Voice Technology: Topic 4: company, voice, tesla, like, also, system, say, technology, one, time\n",
    "- Platform: Topic 5: company, data, said, platform, startup, venture, customer, also, year, new\n",
    "- Machine Learning: Topic 6: data, model, company, learning, machine, robot, like, time, microsoft, one\n",
    "- Human-Technology Systems: Topic 7: data, company, like, research, also, human, google, work, system, say\n",
    "- Social Media: Topic 8: facebook, system, data, also, company, say, people, year, one, platform\n",
    "- Programming: Topic 9: codex, code, openai, food, company, people, one, video, like, say\n",
    "\n",
    "## 2022년\n",
    "- Robotics: Topic 0: robot, company, google, robotics, like, system, new, year, also, one\n",
    "- Data-Driven Analysis: Topic 1: data, model, like, also, carbon, clearview, one, work, say, company\n",
    "- Language Models: Topic 2: language, model, data, google, company, like, say, system, new, tool\n",
    "- Startup: Topic 3: company, data, said, startup, also, like, technology, year, new, platform\n",
    "- Platform: Topic 4: data, company, nvidia, said, platform, also, new, time, say, year\n",
    "- Supply Chain: Topic 5: microsoft, data, image, skin, business, use, startup, supply, say, chain\n",
    "- Human-AI Interaction: Topic 6: mem, say, human, machine, company, like, musk, robot, people, one\n",
    "- Customer Service: Topic 7: company, startup, customer, said, data, platform, service, new, year, capital\n",
    "- Generative AI: Topic 8: image, model, like, system, text, art, video, diffusion, work, make\n",
    "- Human-Technology Systems: Topic 9: company, data, like, user, also, new, say, use, technology, startup\n",
    "\n",
    "## 2023년\n",
    "- EMS: Topic 0: user, data, company, also, tool, like, new, microsoft, say, use\n",
    "- Chat GPT: Topic 1: openai, chatgpt, microsoft, model, gpt, copilot, company, user, like, text\n",
    "- Generative AI: Topic 2: model, data, company, generative, also, said, risk, open, source, like\n",
    "- Online Communities: Topic 3: think, like, thing, going, people, one, way, really, lot, get\n",
    "- OpenAI Governance: Topic 4: openai, altman, microsoft, board, company, ceo, said, new, team, sam\n",
    "- Google Search: Topic 5: google, new, search, like, tool, company, also, content, user, image\n",
    "- Search Engine: Topic 6: google, new, feature, microsoft, bing, like, also, user, search, app\n",
    "- Music Tools: Topic 7: music, company, artist, like, work, new, tool, year, one, song\n",
    "- Startup: Topic 8: company, model, generative, startup, image, data, like, said, new, customer\n",
    "- Voice Technology: Topic 9: voice, company, like, people, musk, one, said, time, would, say\n",
    "\n",
    "## 2024년\n",
    "- Online Communities: Topic 0: think, people, going, thing, way, like, right, really, work, get\n",
    "- Startup: Topic 1: startup, data, company, model, said, year, also, new, million, generative\n",
    "- Product: Topic 2: model, like, company, one, customer, product, data, said, time, say\n",
    "- Apple: Topic 3: apple, image, video, new, feature, like, app, also, user, device\n",
    "- Sustainability Tech: Topic 4: arm, pin, waste, meeting, rabbit, humane, recall, otter, greyparrot, recycling\n",
    "- Google Gemini: Topic 5: google, gemini, search, feature, user, new, model, like, also, generative\n",
    "- Enterprise AI: Topic 6: model, microsoft, openai, copilot, data, gpt, open, company, training, developer\n",
    "- Tech Personalities: Topic 7: chromebook, velastegui, zuckerberg, supermaven, superannotate, fandom, wohl, cable, defcon, wendy\n",
    "- Big Tech Company: Topic 8: openai, meta, chatgpt, company, also, safety, board, said, altman, content\n",
    "- Content Platform: Topic 9: content, like, voice, company, one, people, said, user, news, use\n",
    "\n",
    "## 2025년\n",
    "- Online Communities: Topic 0: think, going, like, people, company, make, want, one, get, way\n",
    "- Chatbot: Topic 1: model, openai, google, gemini, chatgpt, company, user, deepseek, like, new\n",
    "- Language Models: Topic 2: model, company, grok, said, data, think, also, like, product, language\n",
    "- Tech Personalities: Topic 3: company, google, openai, musk, said, billion, trump, startup, also, tech\n",
    "- AI Search: Topic 4: data, google, perplexity, company, also, say, said, search, use, startup\n",
    "- Hardware AI: Topic 5: nvidia, tesla, rtx, company, musk, blackwell, also, xai, chip, new\n",
    "- Enterprise AI: Topic 6: microsoft, model, meta, company, data, openai, copilot, chatgpt, new, user\n",
    "- AI Agents: Topic 7: openai, agent, company, alexa, amazon, said, new, like, system, also\n",
    "- Apple: Topic 8: apple, feature, new, device, intelligence, also, company, model, game, humane\n",
    "- Multimedia Tech: Topic 9: one, like, video, rtx, think, new, thing, google, want, get"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:35:17.896458Z",
     "start_time": "2025-05-02T03:35:16.234172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 연도별 토픽 레이블 딕셔너리\n",
    "topic_label_mappings = {\n",
    "    2021: {\n",
    "        0: \"Google Search\",\n",
    "        1: \"Facial Recognition\",\n",
    "        2: \"Customer Features\",\n",
    "        3: \"Startup\",\n",
    "        4: \"Voice Technology\",\n",
    "        5: \"Platform\",\n",
    "        6: \"Machine Learning\",\n",
    "        7: \"Human-Technology Systems\",\n",
    "        8: \"Social Media\",\n",
    "        9: \"Programming\"\n",
    "    },\n",
    "    2022: {\n",
    "        0: \"Robotics\",\n",
    "        1: \"Data-Driven Analysis\",\n",
    "        2: \"Language Models\",\n",
    "        3: \"Startup\",\n",
    "        4: \"Platform\",\n",
    "        5: \"Supply Chain\",\n",
    "        6: \"Human-AI Interaction\",\n",
    "        7: \"Customer Service\",\n",
    "        8: \"Generative AI\",\n",
    "        9: \"Human-Technology Systems\"\n",
    "    },\n",
    "    2023: {\n",
    "        0: \"EMS\",\n",
    "        1: \"Chat GPT\",\n",
    "        2: \"Generative AI\",\n",
    "        3: \"Online Communities\",\n",
    "        4: \"OpenAI Governance\",\n",
    "        5: \"Google Search\",\n",
    "        6: \"Search Engine\",\n",
    "        7: \"Music Tools\",\n",
    "        8: \"Startup\",\n",
    "        9: \"Voice Technology\"\n",
    "    },\n",
    "    2024: {\n",
    "        0: \"Online Communities\",\n",
    "        1: \"Startup\",\n",
    "        2: \"Product\",\n",
    "        3: \"Apple\",\n",
    "        4: \"Sustainability Tech\",\n",
    "        5: \"Google Gemini\",\n",
    "        6: \"Enterprise AI\",\n",
    "        7: \"Tech Personalities\",\n",
    "        8: \"Big Tech Company\",\n",
    "        9: \"Content Platform\"\n",
    "    },\n",
    "    2025: {\n",
    "        0: \"Online Communities\",\n",
    "        1: \"Chatbot\",\n",
    "        2: \"Language Models\",\n",
    "        3: \"Tech Personalities\",\n",
    "        4: \"AI Search\",\n",
    "        5: \"Hardware AI\",\n",
    "        6: \"Enterprise AI\",\n",
    "        7: \"AI Agents\",\n",
    "        8: \"Apple\",\n",
    "        9: \"Multimedia Tech\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 대상 연도 리스트\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "# 연도별 처리\n",
    "for year in years:\n",
    "    print(f\"🔄 Processing year: {year}\")\n",
    "\n",
    "    # 1. 기사 데이터 로드 및 필터링\n",
    "    article_df = pd.read_csv('data/01_combined_article.csv')\n",
    "    article_df = article_df[article_df['date'] == year].reset_index(drop=True)\n",
    "\n",
    "    # 2. 해당 연도 토픽 행렬 불러오기\n",
    "    topic_matrix_path = f'data/result/02/02_article_{year}_topic_matrix.csv'\n",
    "    topic_matrix = pd.read_csv(topic_matrix_path)\n",
    "\n",
    "    # 3. 가장 높은 토픽 선택\n",
    "    top_topic = topic_matrix.idxmax(axis=1)\n",
    "    top_topic_num = top_topic.str.extract(r'(\\d+)').astype(int)[0]\n",
    "\n",
    "    # 4. 토픽 라벨 매핑\n",
    "    topic_label_mapping = topic_label_mappings[year]\n",
    "    article_df['topic_label'] = top_topic_num.map(topic_label_mapping)\n",
    "\n",
    "    # 5. 저장\n",
    "    save_path = f'data/result/02/02_article_{year}_labeled.csv'\n",
    "    article_df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ Saved labeled file for {year} to: {save_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing year: 2021\n",
      "✅ Saved labeled file for 2021 to: data/result/02/02_article_2021_labeled.csv\n",
      "🔄 Processing year: 2022\n",
      "✅ Saved labeled file for 2022 to: data/result/02/02_article_2022_labeled.csv\n",
      "🔄 Processing year: 2023\n",
      "✅ Saved labeled file for 2023 to: data/result/02/02_article_2023_labeled.csv\n",
      "🔄 Processing year: 2024\n",
      "✅ Saved labeled file for 2024 to: data/result/02/02_article_2024_labeled.csv\n",
      "🔄 Processing year: 2025\n",
      "✅ Saved labeled file for 2025 to: data/result/02/02_article_2025_labeled.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 데이터 라벨링 작업 - Factor Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2021년\n",
    "- Factor_1: IS 발전 : NLP, GPT-3, 자동 완성, 코드 생성, 딥러닝, 로봇 설계 등 기술적 특성과 시스템 중심 AI 발전 내용을 다룸. 대표 문서들은 대부분 모델링, MLOps, 시스템 구조, 언어 처리 등 기술 구현 중심.\n",
    "- Factor_2: IT 조직 : 조직 차원의 AI 활용, 물류 자동화, 모빌리티 생태계 기획, 기업 보안과 인수합병 등의 전략적 의사결정 이슈에 초점. 예: 물류 스타트업 성장, 정부-기업 간 교통계획, 사이버보안 기업 성장 전략.\n",
    "- Factor_3: IT 시장 : 암호화폐 생태계, 스포츠 스트리밍, 프라이버시를 강조한 플랫폼, 광고 생태계 등 시장 및 산업 구조 변화와 관련된 주제. 예: Tether 논란, Apple 프라이버시 정책, Instagram 청소년 정책 등.\n",
    "- Factor_4: IT 개인 : 개인의 행동, 인식, 감정과 관련된 기술의 영향 분석. 예: Babylon Health의 윤리 논란, 프라이버시 규제, Facebook 내부고발자 이슈 등 개인의 기술 수용성과 규제 반응 중심.\n",
    "- Factor_5: IT 그룹 : 스타트업 생태계와 공동체, 투자자-창업자 네트워크, 기업 간 협업 생태계 등 집단 단위의 상호작용과 커뮤니티 확장 관련 주제. 예: 도시별 테크 허브 분석, 투자자 동향, 협업 기반 혁신 사례 등.\n",
    "\n",
    "## 2022년\n",
    "- Factor_1: IT 조직 : AI 기업의 마케팅 전략, 로보택시 상용화, 산업용 드론, 사이버 보안 등 ‘조직 수준의 AI 기술 적용, 제품화, 내부 역량 구축’에 초점. 기술을 조직 내부 역량과 연결짓거나 기업 전략의 일부로 다룸.\n",
    "- Factor_2: IT 개인 : ChatGPT 사용성, 개인화 의료 플랫폼, 소비자 맞춤형 쇼핑, 언어 검색 판결 사례 등 ‘개인 경험과 반응, 인간-AI 상호작용’에 초점을 둔 문서가 다수. 특히 의료, 소비자 행동, 정책적 반응을 중심으로 개인의 기술 수용성 탐색.\n",
    "- Factor_3: IS 발전 : ChatGPT 기술 자체, 언어모델 안전성, 방송·미디어 혁신 등 ‘AI 기술, 언어모델, 개인화 알고리즘의 진보와 응용’을 다룸. 기술 구조, 알고리즘, 시스템 설계와 같은 IS 기술 발전의 정체성과 부합.\n",
    "- Factor_4: IT 시장 : 뉴스 미디어의 web3 전환, 메타의 다국어 번역, 언론의 혁신 부족 등 ‘AI와 사회, 산업, 미디어 시스템 간 관계 및 정책 영향’을 중심으로 산업/정책 구조와 시장 변화에 초점.\n",
    "- Factor_5: IT 그룹 : 생성형 AI 이미지·영상 툴이 일반 사용자 커뮤니티와 창작자 그룹 내에서 생성·유통되며 만들어내는 집단적 창작 경험에 대한 논의. AI 기반 예술 도구의 확산이 협업과 집단 창작 생태계에 미치는 영향 강조.\n",
    "\n",
    "## 2023년\n",
    "- Factor_1: IT 조직 : Adobe, OpenAI, EvenUp, CoreWeave 등 기업 중심의 기술 개발과 전략적 실행을 다룬 기사들이 주를 이룸. 예: Firefly 생성형 모델 출시, 법률 자동화, 클라우드 인프라 전환 등 조직의 IT 활용과 기술 내재화 전략이 핵심.\n",
    "- Factor_2: IT 시장 : 유럽연합 AI 법안, 캘리포니아 AI 규제 등 거버넌스와 시장 규칙 설정을 둘러싼 정책, 산업 동향 기사 다수. 이는 시장 구조 변화와 산업 간 상호작용이라는 'IT 시장'의 정의와 부합.\n",
    "- Factor_3: IT 개인 : AI 기술의 사회적 수용, ChatGPT의 사용자 영향, API 활용 사례 등 개인의 행동·인지·경험을 중심으로 한 내용. 개인화된 상호작용 및 기술 수용성 문제를 다룸.\n",
    "- Factor_4: IS 발전 : GPT-4, GPT-4 Turbo 등 멀티모달 언어모델의 기술적 진보, API 출시, 모델 성능 논의 등 시스템 아키텍처 및 기능성 중심의 기술 발전 기사 중심.\n",
    "- Factor_5: IT 그룹 : Clearview AI, Squarespace, Harvard 교수, 음악 산업 CEO 등의 팟캐스트 및 대담 중심 기사로, 협업 커뮤니티, 윤리, 저작권, 창작 집단 등 다양한 집단의 역할과 상호작용을 조명.\n",
    "\n",
    "## 2024년\n",
    "- Factor_1: IT 조직 : Apple, Spotify, Amazon, Adobe 등 대형 기술 기업들이 자사 제품과 서비스에 AI 기능을 전략적으로 도입한 사례가 중심. 예: WWDC에서 발표한 Apple의 AI 계획, Adobe의 Firefly, Amazon과 Spotify의 AI 추천 시스템 도입 등은 조직 차원의 기술 내재화와 사업 전략으로 해석됨.\n",
    "- Factor_2: IS 발전 : DevSecOps 트렌드, 수리 진단을 위한 음향 기반 AI, 신약 설계, 특허 분석 플랫폼 등은 모두 알고리즘·하드웨어·데이터 기반 시스템 혁신 사례. 이는 기술적 구조와 성능 향상이라는 전통적 IS 발전의 정의에 부합함.\n",
    "- Factor_3: IT 시장 : Meta의 Llama 3, Mistral AI, xAI, AI2의 오픈모델 공개 등은 AI 모델 경쟁 구도 속에서의 시장 주도권 확보 전략. 오픈소스 생태계 형성과 기술 리더십 경쟁은 산업 및 플랫폼 시장의 역학을 반영함.\n",
    "- Factor_4: IT 개인 : Google Gemini 기반 크롬 브라우저 기능, AI 멀티서치, 개인 맞춤형 기능 통합 등은 사용자의 웹 경험을 직접적으로 변화시키는 기술. 이는 개인의 수용성, 사용성, UX 변화 등 ‘IT와 개인의 상호작용’ 중심.\n",
    "- Factor_5: IT 그룹 : 정치 딥페이크, 유명인 생성 콘텐츠 논란, 팬덤 집단의 대응, 아동 보호 관련 규제 등은 모두 집단적 대응, 사회 집합체 내 규범/신뢰/협력 구조와 관련. AI가 집단 간 상호작용 및 사회 질서에 미치는 영향 분석으로 분류됨.\n",
    "\n",
    "## 2025년\n",
    "- Factor_1: IS 발전 : Google Gemini, GPT-4.5, DeepSeek 등 주요 생성형 AI 모델과 기술 스택의 비교, 구조, 훈련 방식 등에 관한 기사 중심. 모델 성능, 기능 개선, AI 훈련 데이터 등 기술적 시스템 발전에 초점을 두고 있어 IS 발전으로 분류됨.\n",
    "- Factor_2: IT 조직 : Meta의 저작권 소송, 기업 내부 결정 과정(예: Zuckerberg의 LLaMA 훈련 승인), Fiverr의 업무 자동화 전략, 아동 온라인 안전을 위한 조직 간 협업 등. 조직 수준에서의 의사결정, 전략 수립, 법적 대응이 중심.\n",
    "- Factor_3: IT 시장 : 유럽의 오픈소스 LLM 전략, 공공 이익 기반 AI 생태계 구축, Gemma 3의 라이선스 문제, EU AI 법안 등 시장·산업 단위에서의 거버넌스, 경쟁, 규제, 생태계 구성 논의로 시장 관점에 적합.\n",
    "- Factor_4: IT 개인 : AI 안전성 논쟁(AI doom), 정치 및 기술 규제에 대한 여론 반응, 대형 기업 인수에 따른 사용자 및 사회적 수용, 투자 심리 등 개인 수준에서 기술 변화에 대한 인지·행동·감정 반응 중심.\n",
    "- Factor_5: IT 그룹 : Splice, Dow Jones, UiPath, Omi와 같은 다양한 산업 내 조직 리더와 기술 창업자들의 AI 도입·윤리·표현의 자유 등에 관한 인터뷰 중심. 이는 기업 내부 집단, 커뮤니티, 이해관계자 집단 간 상호작용과 신뢰 이슈를 반영하여 그룹 단위에 해당.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T02:34:24.359481Z",
     "start_time": "2025-05-03T02:34:24.355382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "factor_label_mappings = {\n",
    "    2021: {\n",
    "        'Factor_1': 'IS 발전',\n",
    "        'Factor_2': 'IT 조직',\n",
    "        'Factor_3': 'IT 시장',\n",
    "        'Factor_4': 'IT 개인',\n",
    "        'Factor_5': 'IT 그룹'\n",
    "    },\n",
    "    2022: {\n",
    "        'Factor_1': 'IT 조직',\n",
    "        'Factor_2': 'IT 개인',\n",
    "        'Factor_3': 'IS 발전',\n",
    "        'Factor_4': 'IT 시장',\n",
    "        'Factor_5': 'IT 그룹'\n",
    "    },\n",
    "    2023: {\n",
    "        'Factor_1': 'IT 조직',\n",
    "        'Factor_2': 'IT 시장',\n",
    "        'Factor_3': 'IT 개인',\n",
    "        'Factor_4': 'IS 발전',\n",
    "        'Factor_5': 'IT 그룹'\n",
    "    },\n",
    "    2024: {\n",
    "        'Factor_1': 'IT 조직',\n",
    "        'Factor_2': 'IS 발전',\n",
    "        'Factor_3': 'IT 시장',\n",
    "        'Factor_4': 'IT 개인',\n",
    "        'Factor_5': 'IT 그룹'\n",
    "    },\n",
    "    2025: {\n",
    "        'Factor_1': 'IS 발전',\n",
    "        'Factor_2': 'IT 조직',\n",
    "        'Factor_3': 'IT 시장',\n",
    "        'Factor_4': 'IT 개인',\n",
    "        'Factor_5': 'IT 그룹'\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T02:37:23.913751Z",
     "start_time": "2025-05-03T02:37:23.842186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "year = 2021\n",
    "\n",
    "# 파일 경로\n",
    "labeled_path = f'data/result/02/02_article_{year}_labeled.csv'\n",
    "factor_scores_path = f'data/result/02/02_article{year}_factor_scores.csv'\n",
    "\n",
    "# 데이터 불러오기\n",
    "labeled_df = pd.read_csv(labeled_path)\n",
    "factor_scores = pd.read_csv(factor_scores_path)\n",
    "\n",
    "# 가장 높은 점수의 Factor 선택 및 라벨 매핑\n",
    "factor_columns = ['Factor_1', 'Factor_2', 'Factor_3', 'Factor_4', 'Factor_5']\n",
    "factor_scores['max_factor'] = factor_scores[factor_columns].idxmax(axis=1)\n",
    "factor_scores['assigned_label'] = factor_scores['max_factor'].map(factor_label_mappings[year])  # ✅ 수정\n",
    "\n",
    "# 병합\n",
    "merged_df = pd.concat([\n",
    "    labeled_df.reset_index(drop=True),\n",
    "    factor_scores[['max_factor', 'assigned_label']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# 저장\n",
    "merged_df.to_csv(f'data/result/02/02_article_{year}_merged_labeled.csv', index=False)\n",
    "print(merged_df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  date  \\\n",
      "0  Walmart is expanding its robot-powered fulfill...  2021   \n",
      "1    Google is investigating another top AI ethicist  2021   \n",
      "2  Amazon opens Alexa AI tech for the first time ...  2021   \n",
      "3  FTC settles with photo storage app that pivote...  2021   \n",
      "4                   Why Google’s union is a big deal  2021   \n",
      "\n",
      "                                             content  \\\n",
      "0  Walmart is planning to increase the number of ...   \n",
      "1  Google is investigating artificial intelligenc...   \n",
      "2  Amazon will now allow third-party companies th...   \n",
      "3  The Federal Trade Commission has reached a set...   \n",
      "4  On January 4th, 2021, Google workers and contr...   \n",
      "\n",
      "                                            keywords affiliations  \\\n",
      "0                    AI, Business, News, Robot, Tech        verge   \n",
      "1                             AI, Google, News, Tech        verge   \n",
      "2  AI, Amazon, Amazon Alexa, Business, Cars, News...        verge   \n",
      "3                    AI, News, Policy, Privacy, Tech        verge   \n",
      "4                  AI, Featured Videos, Google, Tech        verge   \n",
      "\n",
      "          topic_label max_factor assigned_label  \n",
      "0            Platform   Factor_2          IT 조직  \n",
      "1       Google Search   Factor_4          IT 개인  \n",
      "2       Google Search   Factor_1          IS 발전  \n",
      "3  Facial Recognition   Factor_5          IT 그룹  \n",
      "4       Google Search   Factor_4          IT 개인  \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T02:37:27.140519Z",
     "start_time": "2025-05-03T02:37:26.621537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 병합할 파일들의 년도 리스트\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "# 모든 파일을 담을 리스트\n",
    "dfs = []\n",
    "\n",
    "# 각 년도별 파일을 읽어서 리스트에 추가\n",
    "for year in years:\n",
    "    file_path = f'data/result/02/02_article_{year}_merged_labeled.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['year'] = year  # 혹시 year 컬럼 없으면 추가\n",
    "    dfs.append(df)\n",
    "\n",
    "# 모든 데이터프레임을 하나로 합치기\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 결과 저장\n",
    "merged_df.to_csv('data/result/02/02_article_2021_2025_merged_labeled.csv', index=False)\n",
    "\n",
    "print('✅ 저장 완료: data/result/02/02_article_2021_2025_merged_labeled.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: data/result/02/02_article_2021_2025_merged_labeled.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
